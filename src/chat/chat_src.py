import discord
from discord.ext import commands
import logging
import json
from src.chat.chat_env import system_prompt , SYSTEM_PROMPT
from src.config import ADMIN_IDS
from src.chat.gemini_format import gemini_format_callback
from src.config import LLM_FORMAT , LLM_ALLOW_CHANNELS ,ADMIN_IDS
from src.chat.chat_aux import parse_message_history_to_prompt
from src.llm import LLM
from src.summary.summary_aux import RateLimitingScheduler
import time
class LLM_Chat(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
        self.llm = LLM()
        self.scheduler = RateLimitingScheduler(bot)
        self.log = logging.getLogger("LLM_Chat")
    
    def cog_load(self):
            logging.debug("LLM Chat is starting.")
            self.scheduler_task = self.bot.loop.create_task(self.scheduler.run())
            self.cache_task = self.bot.loop.create_task(self.scheduler.queue_cache_event())
    
    def cog_unload(self):
        logging.debug("LLM Chat is being cancelled.")
        if self.scheduler_task:
            self.scheduler_task.cancel()
        if self.cache_task:
            self.cache_task.cancel()

    def is_channel_allowed(self, channel):
        """æ£€æŸ¥é¢‘é“æƒé™ï¼ˆæ”¯æŒå­åŒºï¼‰"""
        if channel.id in LLM_ALLOW_CHANNELS:
            return True
        if hasattr(channel, "parent_id") and channel.parent_id in LLM_ALLOW_CHANNELS:
            return True
        return False

    def is_triggered(self, message: discord.Message) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦è§¦å‘å›žå¤é€»è¾‘ï¼š
        1. æ¶ˆæ¯ä¸­æåŠäº† Bot (At)
        2. æ¶ˆæ¯å›žå¤äº† Bot çš„æ¶ˆæ¯
        """
        # æƒ…å†µ1: ç›´æŽ¥ At
        if self.bot.user in message.mentions:
            return True
        
        # æƒ…å†µ2: å›žå¤é“¾æ£€æµ‹
        if message.reference and message.reference.cached_message:
            # å¦‚æžœèƒ½èŽ·å–åˆ°ç¼“å­˜çš„æ¶ˆæ¯å¯¹è±¡ï¼Œç›´æŽ¥åˆ¤æ–­ä½œè€…
            if message.reference.cached_message.author.id == self.bot.user.id:
                return True
        elif message.reference:
            # å¦‚æžœæ²¡æœ‰ç¼“å­˜ï¼ˆæ¯”å¦‚æ¶ˆæ¯å¤ªä¹…è¿œï¼‰ï¼Œå°è¯•é€šè¿‡ resolve (APIè°ƒç”¨å¯èƒ½è¾ƒæ…¢ï¼Œé€šå¸¸ message.reference.resolved æ›´å¥½)
            # ç®€å•èµ·è§ï¼Œè¿™é‡Œä¸»è¦ä¾èµ– message.mentionsï¼Œå›žå¤é€šå¸¸ä¼´éš mention
            # å¦‚æžœéœ€è¦å¼ºå›žå¤æ£€æµ‹ï¼ˆå³ä½¿å›žå¤æ—¶å…³æŽ‰äº† mentionï¼‰ï¼Œéœ€è¦ fetch_messageï¼Œä½†ä¼šå¢žåŠ  API æ¶ˆè€—
            pass
            
        return False

    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        # 1. åŸºç¡€è¿‡æ»¤ï¼šä¸å›žå¤è‡ªå·±ï¼Œä¸å›žå¤å…¶ä»–æœºå™¨äºº
        if message.author.bot:
            return

        # 2. é¢‘é“é‰´æƒ
        if not self.is_channel_allowed(message.channel):
            return

        # 3. è§¦å‘æ£€æµ‹
        if not self.is_triggered(message):
            return

        # --- å¼€å§‹å¤„ç† ---
        try:
            # æ˜¾ç¤º "æ­£åœ¨è¾“å…¥..." çŠ¶æ€ï¼Œæå‡ç”¨æˆ·ä½“éªŒ
            async with message.channel.typing():
                
                # 4. èŽ·å–ä¸Šä¸‹æ–‡
                # èŽ·å–æœ€è¿‘çš„ 30 æ¡æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡ (åŒ…å«å½“å‰è¿™æ¡è§¦å‘æ¶ˆæ¯)
                # history è¿”å›žçš„æ˜¯å€’åºçš„ (æœ€æ–°çš„åœ¨å‰)ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒæ­£åºæŽ’åˆ—
                history_messages = [msg async for msg in message.channel.history(limit=30)]
                history_messages.reverse() 

                # 5. æž„å»º Prompt (å¤ç”¨ summary çš„æ ¸å¿ƒå‡½æ•°)
                # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦ä¼ å…¥ members åˆ—è¡¨ï¼Œå› ä¸ºèŠå¤©æ¨¡å¼ä¸‹æˆ‘ä»¬éœ€è¦çœ‹åˆ°æ‰€æœ‰äººçš„å‘è¨€
                
                if LLM_FORMAT == "gemini":
                    final_data = await parse_message_history_to_prompt(
                        message=history_messages,
                        post_processing_callback=gemini_format_callback,
                        bot_user=self.bot.user,
                        admin_ids=ADMIN_IDS
                    )
                    payload_list = final_data.get("contents", [])

                    gemini_system = {
                        "systemInstruction": {
                            "parts": [{"text": SYSTEM_PROMPT}]
                        }
                    }
                    # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬å°† system prompt ä¼ªè£…æˆç¬¬ä¸€æ¡æ¶ˆæ¯ï¼Œç¨åŽåœ¨ llm.py ä¸­æå–
                    payload_list.insert(0, gemini_system)
                    
                else:
                    # OpenAI æ ¼å¼
                    final_prompt = await parse_message_history_to_prompt(
                        message=history_messages,
                        post_processing_callback=gemini_format_callback,
                        bot_user=self.bot.user,
                        admin_ids=ADMIN_IDS
                    )
                    payload_list = final_prompt.get("messages", [])
                    system_payload = {"role": "system", "content": SYSTEM_PROMPT}
                    payload_list.insert(0, system_payload)

                # 6. è°ƒç”¨ LLM
                llm_input_json = json.dumps(payload_list, ensure_ascii=False)
                self.log.debug(f"Chat Playload:\n{llm_input_json}")
                start_ts = time.time()
                llm_result = await self.llm.llm_call(llm_input_json)
                response_chunks = llm_result["chunks"]
                usage = parse_gemini_usage(llm_result["raw"])
                input_tokens = usage["input_tokens"]
                output_tokens = usage["output_tokens"]
                elapsed = time.time() - start_ts
                latency_s = round(elapsed, 3)

                if not response_chunks:
                    await message.reply("*(...ä¼¼ä¹Žé™·å…¥äº†æ²‰æ€ï¼Œæ²¡æœ‰å›žåº”...)*")
                    return

                # 7. å‘é€å›žå¤
                # é€šå¸¸èŠå¤©å›žå¤æ¯”è¾ƒçŸ­ï¼Œå–ç¬¬ä¸€ä¸ª chunk å³å¯ã€‚
                # å¦‚æžœå¾ˆé•¿ï¼Œå¯ä»¥åˆ†æ®µå‘é€
                reply_content = response_chunks[0] + f"\n\n-# Time:{latency_s}s | In :{input_tokens}t | Out :{output_tokens}t"
                
                if "System Seed:" in reply_content:
                    reply_content = reply_content.split("System Seed:")[0]

                # å›žå¤ç”¨æˆ·
                await message.reply(reply_content, mention_author=False)

        except Exception as e:
            self.log.error(f"Chat processing error: {e}", exc_info=True)
            # èŠå¤©æ¨¡å¼ä¸‹å‡ºé”™é€šå¸¸ä¸å‘æŠ¥é”™ä¿¡æ¯ç»™ç”¨æˆ·ï¼Œä»¥å…æ‰“æ–­æ²‰æµ¸æ„Ÿï¼Œæˆ–è€…å‘ä¸ªç®€å•çš„è¡¨æƒ…
            await message.add_reaction("ðŸ˜µ")

def parse_gemini_usage(resp: dict) -> dict:
    usage = resp.get("usageMetadata", {})

    return {
        "input_tokens": usage.get("promptTokenCount", 0),
        "output_tokens": usage.get("candidatesTokenCount", 0),
        "thought_tokens": usage.get("thoughtsTokenCount", 0),
        "total_tokens": usage.get("totalTokenCount", 0),
    }