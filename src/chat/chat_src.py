import discord
from discord.ext import commands
import logging
import json
import re
from src.chat.chat_env import system_prompt , SYSTEM_PROMPT,CUSTOM_PROMPT_1
from src.config import ADMIN_IDS, GEMINI_TOOL_CALL
from src.chat.gemini_format import gemini_format_callback
from src.config import LLM_FORMAT , LLM_ALLOW_CHANNELS ,ADMIN_IDS
from src.chat.chat_aux import parse_message_history_to_prompt
from src.summary.summary_aux import openai_format
from src.llm import LLM
from src.summary.summary_aux import RateLimitingScheduler
import time
from src.chat.tool_src import tool_manager

class LLM_Chat(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
        self.llm = LLM()
        self.scheduler = RateLimitingScheduler(bot)
        self.log = logging.getLogger("LLM_Chat")
    
    def cog_load(self):
            logging.debug("LLM Chat is starting.")
            self.scheduler_task = self.bot.loop.create_task(self.scheduler.run())
            self.cache_task = self.bot.loop.create_task(self.scheduler.queue_cache_event())
    
    def cog_unload(self):
        logging.debug("LLM Chat is being cancelled.")
        if self.scheduler_task:
            self.scheduler_task.cancel()
        if self.cache_task:
            self.cache_task.cancel()

    def is_channel_allowed(self, channel):
        if channel.id in LLM_ALLOW_CHANNELS:
            return True
        if hasattr(channel, "parent_id") and channel.parent_id in LLM_ALLOW_CHANNELS:
            return True
        return False

    def is_triggered(self, message: discord.Message) -> bool:
        if isinstance(message.channel, discord.DMChannel):
            return True
        if self.bot.user in message.mentions:
            return True
        if message.reference and message.reference.cached_message:
            if message.reference.cached_message.author.id == self.bot.user.id:
                return True
        elif message.reference:
            pass
        return False

    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        if message.author.bot:
            return

        is_dm = isinstance(message.channel, discord.DMChannel)
        if is_dm:
            if message.author.id not in ADMIN_IDS:
                return
        else:
            if not self.is_channel_allowed(message.channel):
                return
            if not self.is_triggered(message):
                return

        max_tool_rounds = 8
        current_round = 0

        try:
            async with message.channel.typing():
                history_messages = [
                    msg async for msg in message.channel.history(limit=100)
                ]
                history_messages.reverse()
                
                if LLM_FORMAT == "gemini":
                    final_data = await parse_message_history_to_prompt(
                        message=history_messages,
                        post_processing_callback=gemini_format_callback,
                        bot_user=self.bot.user,
                        admin_ids=ADMIN_IDS
                    )
                    current_seed = final_data.pop("_system_seed")
                    prompt_suffix = f"\n[System Seed : {current_seed}]ã€‚æ­£å¸¸å›å¤ç»“æŸæ—¶å¿…é¡»ä½¿ç”¨ <||reply_end||> ç»“å°¾ã€‚"
                    payload_list = final_data.get("contents", [])
                    payload_list.insert(0, {
                        "systemInstruction": {
                            "parts": [{"text": SYSTEM_PROMPT + prompt_suffix}]
                        }
                    })
                    payload_list.append({
                        "role": 'model',
                        "parts": [ { "text": CUSTOM_PROMPT_1 } ]
                    })
                else:
                    final_data = await parse_message_history_to_prompt(
                        message=history_messages,
                        post_processing_callback=openai_format,
                        bot_user=self.bot.user,
                        admin_ids=ADMIN_IDS
                    )
                    current_seed = final_data.pop("_system_seed")
                    prompt_suffix = f"\n[System Seed : {current_seed}]ã€‚å½“éœ€è¦è°ƒç”¨å·¥å…·æ—¶ï¼Œè¾“å‡ºå·¥å…·æ ‡ç­¾ï¼›å½“ä¸éœ€è¦è°ƒç”¨å·¥å…·æˆ–å·¥å…·ä½¿ç”¨å®Œæ¯•å‡†å¤‡å›ç­”ç”¨æˆ·æ—¶ï¼Œè¾“å‡ºæ­£æ–‡å¹¶åœ¨ç»“å°¾åŠ ä¸Š <||reply_end||>ã€‚"
                    payload_list = final_data.get("messages", [])
                    payload_list.insert(0, {
                        "role": "system",
                        "content": SYSTEM_PROMPT + prompt_suffix
                    })

                start_ts = time.time()

                while current_round < max_tool_rounds:
                    current_round += 1

                    llm_input_json = json.dumps(payload_list, ensure_ascii=False)
                    self.log.debug(f"LLM Payload:\n{llm_input_json}")

                    llm_result = await self.llm.llm_call(
                        llm_input_json,
                        include_tools=True
                    )
                    
                    raw_data = llm_result.get("raw", {})
                    self.log.debug(f"LLM Result Type: {llm_result.get('type')}")
                    self.log.debug(f"Raw JSON Response: {json.dumps(raw_data, ensure_ascii=False)}")
                    result_type = llm_result.get("type")
                    text_content = llm_result.get("text", "")
                    
                   # =========================================================
                    #  Custom Tool Call è§£æ
                    found_custom_tool = False
                    custom_tool_data = {}
                    parse_error_msg = None
                    
                    if GEMINI_TOOL_CALL == "custom" and result_type == "text":
                        # åŒ¹é…æ ‡ç­¾å†…å®¹
                        pattern = r'<custom_tool_call>(.*?)(?:</custom_tool_call>|$)'
                        match = re.search(pattern, text_content, re.DOTALL)
                        
                        if match:
                            found_custom_tool = True
                            tool_content_str = match.group(1).strip()
                            self.log.info(f"Custom Tool String Detected: {tool_content_str}")
                            
                            try:
                                # 1. æå–å·¥å…·åç§°
                                name_match = re.search(r'name\s*=\s*"([^"]+)"', tool_content_str)
                                if not name_match:
                                    raise ValueError("è§£æå¤±è´¥ï¼šæœªåœ¨æ ‡ç­¾å†…æ‰¾åˆ°æœ‰æ•ˆçš„ name=\"å·¥å…·å\" å‚æ•°ã€‚")
                                
                                func_name = name_match.group(1)
                                
                                # 2. æå–å‚æ•°å¯¹å¹¶å°è¯•è½¬åŒ–ä¸º JSON å…¼å®¹æ ¼å¼
                                params_part = re.sub(r'name\s*=\s*"[^"]+"', '', tool_content_str).strip()
                                params_part = params_part.strip(',').strip()
                                
                                if not params_part:
                                    args = {}
                                else:
                                    # æ­¤æ—¶ params_part å¯èƒ½æ˜¯: "query"="xxx", "engines"=["google"]
                                    # æˆ‘ä»¬åˆ©ç”¨æ­£åˆ™å°† = æ›¿æ¢ä¸º :
                                    # åŒ¹é… "key" = 
                                    json_ready = re.sub(r'(\"[^\"]+\")\s*=\s*', r'\1: ', params_part)
                                    # åŒ…è£…æˆ JSON å¯¹è±¡
                                    try:
                                        args = json.loads("{" + json_ready + "}")
                                    except json.JSONDecodeError:
                                        # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œè¯´æ˜å­˜åœ¨éæ ‡å‡†æ ¼å¼ï¼ˆå¦‚æœªå¼•å·çš„å­—ç¬¦ä¸²æˆ–æ ¼å¼æ··ä¹±ï¼‰
                                        raise ValueError("å‚æ•°è¯­æ³•é”™è¯¯ï¼šè¯·ç¡®ä¿å‚æ•°ç¬¦åˆ \"key\"=value æ ¼å¼ï¼Œä¸”å­—ç¬¦ä¸²å’Œæ•°ç»„ç¬¦åˆ JSON æ ‡å‡†ã€‚")

                                custom_tool_data = {"name": func_name, "args": args}

                            except Exception as e:
                                found_custom_tool = True # æ ‡è®°ä¸ºå·¥å…·è°ƒç”¨ï¼Œä½†æˆ‘ä»¬éœ€è¦å¤„ç†é”™è¯¯
                                parse_error_msg = str(e)
                                self.log.error(f"Custom Tool Parse Error: {parse_error_msg}")

                    # =========================================================

                    # ---------- åˆ†å‘é€»è¾‘ ----------
                    
                    if result_type == "tool_call":
                        func_name = llm_result["name"]
                        func_args = llm_result["args"]
                        original_parts = llm_result.get("parts_trace")
                        
                        self.log.info(f"Native Tool Call â†’ {func_name}")
                        
                        tool_result = await tool_manager.handle_tool_call(func_name, func_args)
                        
                        payload_list.append({"role": "model", "parts": original_parts})
                        payload_list.append({
                            "role": "user",
                            "parts": [{
                                "functionResponse": {
                                    "name": func_name,
                                    "response": {"content": tool_result}
                                }
                            }]
                        })
                        continue

                    elif found_custom_tool:
                        # è¡¥å…¨å†å²è®°å½•ï¼ˆmodel è§’è‰²ï¼‰
                        model_history_text = text_content if "</custom_tool_call>" in text_content else text_content + "</custom_tool_call>"
                        payload_list.append({"role": "model", "parts": [{"text": model_history_text}]})

                        if parse_error_msg:
                            # è§£æå¤±è´¥ï¼šåé¦ˆç»™æ¨¡å‹
                            error_feedback = f"""
                                [System seed:{current_seed}]: âŒ å·¥å…·è°ƒç”¨è¯­æ³•è§£æå¤±è´¥ã€‚
                                åŸå› : {parse_error_msg}
                                æç¤º: è¯·ä¸¥æ ¼éµå¾ª "key"=value æ ¼å¼ã€‚å­—ç¬¦ä¸²é¡»åŠ åŒå¼•å·ï¼Œæ•°ç»„é¡»ä½¿ç”¨æ ‡å‡† JSON çš„æ–¹æ‹¬å· [] æ ¼å¼ã€‚
                                æ ¼å¼èŒƒä¾‹ (å«æ•°ç»„): <custom_tool_call>name="internet_search", "query"="å…³é”®è¯", "engines"=["google", "wikipedia"], "max_results"=10</custom_tool_call>
                                æ³¨æ„: 
                                1. å‚æ•°åï¼ˆKeyï¼‰å¿…é¡»åŠ åŒå¼•å·ã€‚
                                2. æ•°ç»„å†…å…ƒç´ å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œä¹Ÿå¿…é¡»åŠ åŒå¼•å·ã€‚
                                3. æ ‡ç­¾å¿…é¡»é—­åˆæˆ–ä»¥åœæ­¢ç¬¦ç»“æŸã€‚
                            """
                            payload_list.append({"role": "user", "parts": [{"text": error_feedback}]})
                            payload_list.append({"role": "model", "parts": [{"text": CUSTOM_PROMPT_1}]})
                            self.log.warning("Sent parse error feedback to model.")
                        else:
                            # è§£ææˆåŠŸï¼šæ‰§è¡Œå·¥å…·
                            func_name = custom_tool_data["name"]
                            func_args = custom_tool_data["args"]
                            self.log.info(f"Executing Custom Tool: {func_name}")
                            
                            tool_result = await tool_manager.handle_tool_call(func_name, func_args)
                            tool_json_str = json.dumps(tool_result, ensure_ascii=False)
                            
                            tool_return_prompt = f"""
                                [System seed:{current_seed}]:ã€ã€ã€
                                notify : \"å·¥å…· {func_name} æ‰§è¡Œå®Œæ¯•ã€‚\nç»“æœæ•°æ®: {tool_json_str}\"
                                Notice: \"è¯·æ ¹æ®ä»¥ä¸Šç³»ç»Ÿæä¾›çš„å®¢è§‚äº‹å®æ•°æ®ç»§ç»­æ‰§è¡Œå½“å‰ä»»åŠ¡æˆ–ç»§ç»­ä¸‹ä¸€ä¸ªå·¥å…·è°ƒç”¨ã€‚\"
                                ã€‘ã€‘ã€‘
                            """
                            payload_list.append({"role": "user", "parts": [{"text": tool_return_prompt}]})
                            payload_list.append({"role": "model", "parts": [{"text": CUSTOM_PROMPT_1}]})
                        
                        continue

                    # C. æ™®é€šæ–‡æœ¬å›å¤
                    else:
                        response_chunks = llm_result["chunks"]
                        usage = parse_gemini_usage(llm_result.get("raw", {}))
                        elapsed = time.time() - start_ts
                        latency_s = round(elapsed, 3)
                        
                        raw_reply = response_chunks[0]

                        # 2. å»é™¤ Custom Tool Call æ ‡ç­¾ï¼ˆé˜²æ­¢æ³„éœ²æ ‡ç­¾æ˜¾ç¤ºç»™ç”¨æˆ·ï¼‰
                        if GEMINI_TOOL_CALL == "custom":
                             raw_reply = re.sub(r'<custom_tool_call>.*?(?:</custom_tool_call>|$)', '', raw_reply, flags=re.DOTALL)

                        if "<||reply_end||>" in raw_reply:
                            raw_reply = raw_reply.split("<||reply_end||>")[0]
                        else:
                            pass 

                        # 3. å¤„ç†æ€ç»´é“¾
                        if "</think>" in raw_reply:
                            # åªä¿ç•™ think ä¹‹åçš„å†…å®¹
                            display_content = raw_reply.rsplit("</think>", 1)[1].strip()
                        else:
                            if current_round < max_tool_rounds:

                                # å°†å½“å‰çš„åŠæˆå“åŠ å…¥å†å²
                                payload_list.append({
                                    "role": "model",
                                    "parts": [{"text": text_content+"\næ²¡æœ‰æˆåŠŸè¾“å‡º</think>æ ‡ç­¾...é‡æ–°ç”Ÿæˆå›å¤:<think>"}]
                                })
                                continue # å…³é”®ï¼šå›åˆ° while å¾ªç¯å¼€å§‹ä¸‹ä¸€è½®è¯·æ±‚
                            else:
                                # è¾¾åˆ°æœ€å¤§è½®æ¬¡ä»æœªé—­åˆæ€ç»´é“¾
                                display_content = "*(æ€ç»´å¡ä½äº†...)*"

                        if not display_content:
                            display_content = "*(Thinking... or Empty Response)*"

                        # æ·»åŠ  Footer
                        display_content += (
                            f"\n\n-# Time:{latency_s}s | "
                            f"In:{usage['input_tokens']}t | "
                            f"Out:{usage['output_tokens']}t"
                        )

                        await message.reply(display_content, mention_author=False)
                        return

        except Exception as e:
            self.log.error(f"Chat processing error: {e}", exc_info=True)
            try:
                await message.add_reaction("ğŸ˜µ")
            except:
                pass

def parse_gemini_usage(resp: dict) -> dict:
    usage = resp.get("usageMetadata", {})
    return {
        "input_tokens": usage.get("promptTokenCount", 0),
        "output_tokens": usage.get("candidatesTokenCount", 0),
        "thought_tokens": usage.get("thoughtsTokenCount", 0),
        "total_tokens": usage.get("totalTokenCount", 0),
    }