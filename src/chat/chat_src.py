import discord
from discord.ext import commands
import logging
import json
from src.chat.chat_env import system_prompt , SYSTEM_PROMPT,CUSTOM_PROMPT_1
from src.config import ADMIN_IDS
from src.chat.gemini_format import gemini_format_callback
from src.config import LLM_FORMAT , LLM_ALLOW_CHANNELS ,ADMIN_IDS
from src.chat.chat_aux import parse_message_history_to_prompt
from src.llm import LLM
from src.summary.summary_aux import RateLimitingScheduler
import time
from src.chat.tool_src import tool_manager
class LLM_Chat(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
        self.llm = LLM()
        self.scheduler = RateLimitingScheduler(bot)
        self.log = logging.getLogger("LLM_Chat")
    
    def cog_load(self):
            logging.debug("LLM Chat is starting.")
            self.scheduler_task = self.bot.loop.create_task(self.scheduler.run())
            self.cache_task = self.bot.loop.create_task(self.scheduler.queue_cache_event())
    
    def cog_unload(self):
        logging.debug("LLM Chat is being cancelled.")
        if self.scheduler_task:
            self.scheduler_task.cancel()
        if self.cache_task:
            self.cache_task.cancel()

    def is_channel_allowed(self, channel):
        """æ£€æŸ¥é¢‘é“æƒé™ï¼ˆæ”¯æŒå­åŒºï¼‰"""
        if channel.id in LLM_ALLOW_CHANNELS:
            return True
        if hasattr(channel, "parent_id") and channel.parent_id in LLM_ALLOW_CHANNELS:
            return True
        return False

    def is_triggered(self, message: discord.Message) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦è§¦å‘å›å¤é€»è¾‘ï¼š
        1. æ¶ˆæ¯ä¸­æåŠäº† Bot (At)
        2. æ¶ˆæ¯å›å¤äº† Bot çš„æ¶ˆæ¯
        """
        # æƒ…å†µ1: ç›´æ¥ At
        if self.bot.user in message.mentions:
            return True
        
        # æƒ…å†µ2: å›å¤é“¾æ£€æµ‹
        if message.reference and message.reference.cached_message:
            # å¦‚æœèƒ½è·å–åˆ°ç¼“å­˜çš„æ¶ˆæ¯å¯¹è±¡ï¼Œç›´æ¥åˆ¤æ–­ä½œè€…
            if message.reference.cached_message.author.id == self.bot.user.id:
                return True
        elif message.reference:
            # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼ˆæ¯”å¦‚æ¶ˆæ¯å¤ªä¹…è¿œï¼‰ï¼Œå°è¯•é€šè¿‡ resolve (APIè°ƒç”¨å¯èƒ½è¾ƒæ…¢ï¼Œé€šå¸¸ message.reference.resolved æ›´å¥½)
            # ç®€å•èµ·è§ï¼Œè¿™é‡Œä¸»è¦ä¾èµ– message.mentionsï¼Œå›å¤é€šå¸¸ä¼´éš mention
            # å¦‚æœéœ€è¦å¼ºå›å¤æ£€æµ‹ï¼ˆå³ä½¿å›å¤æ—¶å…³æ‰äº† mentionï¼‰ï¼Œéœ€è¦ fetch_messageï¼Œä½†ä¼šå¢åŠ  API æ¶ˆè€—
            pass
            
        return False

    @commands.Cog.listener()
    async def on_message(self, message: discord.Message):
        # ---------- åŸºç¡€è¿‡æ»¤ ----------
        if message.author.bot:
            return

        if not self.is_channel_allowed(message.channel):
            return

        if not self.is_triggered(message):
            return

        max_tool_rounds = 8
        current_round = 0

        try:
            async with message.channel.typing():

                # ---------- 1. æ‹‰å–ä¸Šä¸‹æ–‡ ----------
                history_messages = [
                    msg async for msg in message.channel.history(limit=30)
                ]
                history_messages.reverse()

                # ---------- 2. æ„å»º Prompt ----------
                if LLM_FORMAT == "gemini":
                    final_data = await parse_message_history_to_prompt(
                        message=history_messages,
                        post_processing_callback=gemini_format_callback,
                        bot_user=self.bot.user,
                        admin_ids=ADMIN_IDS
                    )

                    payload_list = final_data.get("contents", [])

                    # Gemini systemInstructionï¼ˆä¼ªè£…ï¼Œåç»­åœ¨ llm_call æ‹†ï¼‰
                    payload_list.insert(0, {
                        "systemInstruction": {
                            "parts": [{"text": SYSTEM_PROMPT}]
                        }
                    })
                    # æ’å…¥æœ€åä¸€æ¡çš„æ¶ˆæ¯
                    payload_list.append({
                        "role": 'model',
                        "parts": [ { "text": CUSTOM_PROMPT_1 } ]
                    })

                else:
                    final_data = await parse_message_history_to_prompt(
                        message=history_messages,
                        post_processing_callback=gemini_format_callback,
                        bot_user=self.bot.user,
                        admin_ids=ADMIN_IDS
                    )

                    payload_list = final_data.get("messages", [])
                    payload_list.insert(0, {
                        "role": "system",
                        "content": SYSTEM_PROMPT
                    })

                # ---------- 3. Tool / LLM å¾ªç¯ ----------
                start_ts = time.time()

                while current_round < max_tool_rounds:
                    current_round += 1

                    llm_input_json = json.dumps(payload_list, ensure_ascii=False)
                    self.log.debug(f"LLM Payload:\n{llm_input_json}")

                    llm_result = await self.llm.llm_call(
                        llm_input_json,
                        include_tools=True
                    )
                    self.log.debug("===== RAW LLM RESULT =====")
                    self.log.debug(llm_result)
                    self.log.debug("===== END RAW LLM RESULT =====")

                    # å³ä½¿ llm_result['type'] æ˜¯ textï¼Œåªè¦ raw é‡Œæœ‰ functionCallï¼Œå°±æ˜¯å·¥å…·è°ƒç”¨
                    raw_candidates = llm_result.get("raw", {}).get("candidates", [])
                    found_function_call_part = None
                    raw_parts_list = []

                    if raw_candidates:
                        raw_parts_list = raw_candidates[0].get("content", {}).get("parts", [])
                        for part in raw_parts_list:
                            if "functionCall" in part:
                                found_function_call_part = part
                                break

                    if found_function_call_part:
                        result_type = "tool_call"
                        
                        func_name = found_function_call_part["functionCall"]["name"]
                        func_args = found_function_call_part["functionCall"]["args"]
                        
                        original_parts = raw_parts_list 
                    else:
                        result_type = llm_result.get("type")
                        original_parts = None


                    # ---------- 4. æ™®é€šæ–‡æœ¬å›å¤ (åªæœ‰æ˜ç¡®ä¸æ˜¯å·¥å…·è°ƒç”¨æ—¶æ‰è¿›è¿™é‡Œ) ----------
                    if result_type == "text":
                        response_chunks = llm_result["chunks"]
                        usage = parse_gemini_usage(llm_result.get("raw", {}))

                        elapsed = time.time() - start_ts
                        latency_s = round(elapsed, 3)

                        reply_content = response_chunks[0]

                        if "System Seed:" in reply_content:
                            reply_content = reply_content.split("System Seed:")[0]

                        # å¤„ç†æ€ç»´é“¾æ ‡ç­¾
                        if "</think>" in reply_content:
                            # è´ªå©ªåŒ¹é…ï¼šå–æœ€åä¸€ä¸ª </think> ä¹‹åçš„å†…å®¹
                            reply_content = reply_content.rsplit("</think>", 1)[1].strip()
                        
                        # å¦‚æœå†…å®¹ä¸ºç©ºï¼ˆæ¯”å¦‚åªæœ‰æ€è€ƒæ²¡æœ‰æ­£æ–‡ï¼‰ï¼Œè¿™é‡Œå¯ä»¥åšä¸€ä¸ªå…œåº•æˆ–è€…æ‰“æ–­
                        # ä½†é€šå¸¸ text ç±»å‹åˆ°è¿™é‡Œå°±è¯¥ç»“æŸäº†
                        if not reply_content:
                            reply_content = "*(æ¨¡å‹ä»…è¾“å‡ºäº†æ€è€ƒè¿‡ç¨‹ï¼Œæœªç”Ÿæˆå›å¤å†…å®¹)*"

                        reply_content += (
                            f"\n\n-# Time:{latency_s}s | "
                            f"In:{usage['input_tokens']}t | "
                            f"Out:{usage['output_tokens']}t"
                        )

                        await message.reply(reply_content, mention_author=False)
                        return

                    # ---------- 5. Tool è°ƒç”¨ ----------
                    if result_type == "tool_call":
                        # æ³¨æ„ï¼šfunc_name å’Œ func_args å·²ç»åœ¨ä¸Šé¢çš„æ£€æµ‹é€»è¾‘ä¸­èµ‹å€¼äº†
                        # å¦‚æœæ²¡æœ‰åœ¨ä¸Šé¢èµ‹å€¼ï¼ˆå³èµ°äº†åŸæœ¬çš„ tool_call åˆ†æ”¯ï¼‰ï¼Œå°è¯•ä» llm_result è·å–
                        if 'func_name' not in locals():
                            func_name = llm_result["name"]
                            func_args = llm_result["args"]
                            original_parts = llm_result.get("parts_trace")

                        self.log.info(f"Tool Call â†’ {func_name}({func_args})")

                        tool_result = await tool_manager.handle_tool_call(
                            func_name,
                            func_args
                        )

                        # å°†åŸå§‹çš„ï¼ˆåŒ…å«æ€è€ƒè¿‡ç¨‹çš„ï¼‰Model å“åº”åŠ å…¥å†å²
                        if original_parts:
                            payload_list.append({
                                "role": "model",
                                "parts": original_parts 
                            })
                        else:
                            # å…œåº•ï¼šå¦‚æœæ²¡æŠ“åˆ° partsï¼Œæ‰‹åŠ¨æ„å»º
                            payload_list.append({
                                "role": "model",
                                "parts": [{
                                    "functionCall": {
                                        "name": func_name,
                                        "args": func_args
                                    }
                                }]
                            })

                        # å¤„ç†å·¥å…·è¿”å›ç»“æœ
                        if tool_result.get("results"):
                            payload_list.append({
                                "role": "function",
                                "parts": [{
                                    "functionResponse": {
                                        "name": func_name,
                                        "response": {
                                            "content": tool_result
                                        }
                                    }
                                }]
                            })
                        else:
                            payload_list.append({
                                "role": "function",
                                "parts": [{
                                    "functionResponse": {
                                        "name": func_name,
                                        "response": {
                                            "content": {
                                                "query": func_args.get("query"),
                                                "results": [],
                                                "notice": "âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…ç»“æœ"
                                            }
                                        }
                                    }
                                }]
                            })

                        continue

                # ---------- 6. å…œåº• ----------
                await message.reply(
                    "*(æ€è€ƒå¥½åƒç»•è¿›æ­»èƒ¡åŒäº†â€¦è¦ä¸æ¢ä¸ªé—®æ³•ï¼Ÿ)*",
                    mention_author=False
                )

        except Exception as e:
            self.log.error(f"Chat processing error: {e}", exc_info=True)
            await message.add_reaction("ğŸ˜µ")


def parse_gemini_usage(resp: dict) -> dict:
    usage = resp.get("usageMetadata", {})

    return {
        "input_tokens": usage.get("promptTokenCount", 0),
        "output_tokens": usage.get("candidatesTokenCount", 0),
        "thought_tokens": usage.get("thoughtsTokenCount", 0),
        "total_tokens": usage.get("totalTokenCount", 0),
    }

async def get_or_create_webhook(channel: discord.TextChannel) -> discord.Webhook:
    webhooks = await channel.webhooks()
    for wh in webhooks:
        if wh.name == "LLM-Chat":
            return wh

    return await channel.create_webhook(name="LLM-Chat")